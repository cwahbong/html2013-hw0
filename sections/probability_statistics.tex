\newcommand{\mlComb}[2]{\prescript{#1}{}{C}_{#2}}
\newcommand{\mlPerm}[2]{\prescript{#1}{}{P}_{#2}}

\section{Probability and Statistics}
\begin{enumerate}[(1)]
	\item (combinatorics) \\
		We prove it by induction. \\
		By definition the equation holds when $N=1$, $1\leq K \leq N$. \\
		If the equation holds for $N = n$, $1 \leq K \leq n$, then for $N = n+1$, \\
		\begin{align*}
			\mlComb{n+1}{K} &= \mlComb{n}{K} + \mlComb{n}{K-1} = \frac{n!}{K!(n-K!)} + \frac{n!}{(K-1)!(n-(K-1))!} \\
					&= \frac{(n-K+1)n! + K\cdot n!}{K!((n+1)-K)!} = \frac{(n+1)!}{K!((n+1)-K)!}
		\end{align*}
	\item (counting)
	\begin{itemize}
		\item $\dfrac{\mlComb{10}{4}}{2^{10}}$
		\item $\dfrac{\mlPerm{13}{2}\cdot \mlComb{4}{3}\cdot \mlComb{4}{2}}{\mlComb{52}{5}}$
	\end{itemize}
	\item (conditional probability) \\
		$\dfrac{1}{\mlComb{3}{1} + \mlComb{3}{2} + \mlComb{3}{3}}$
	\item (Bayes theorem) \\
		$\dfrac{P(X=-1)}{P(|X|=1)} = \dfrac{\frac{1}{8}}{\frac{1}{16} + \frac{1}{8}} = \dfrac{2}{3}$
	\item (union/intersection)
	\begin{itemize}
		\item $\max(P(A\cap B)) = \min(P(A), P(B)) = 0.3$
		\item $\min(P(A\cap B)) = \max(P(A) + P(B) - 1, 0) = 0$
		\item $\max(P(A\cup B)) = \min(P(A) + P(B), 1) = 0.7$
		\item $\min(P(A\cup B)) = \max(P(A), P(B)) = 0.4$
	\end{itemize}
	\item (mean/variance) \\
		Since $\sum^N_{n=1}X_n = N\overline{X}$,
		\begin{align*}
			\sum^N_{n=1}(X_n-\overline{X})^2 &= \sum^N_{n=1}X_n^2 - 2\sum^N_{n=1}X_n\overline{X} + N\overline{X}^2 \\
				&= \sum^N_{n=1}X_n^2 - N\overline{X}^2 = \sum^N_{n=1}(X_n^2 - \overline{X}^2)
		\end{align*}
	\item (Gaussian distribution) \\
		See \url{http://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables}.
		$\overline{Z} = 2 - 3 = -1$, $\sigma^2_{Z} = 1 + 4 = 5$.
\end{enumerate}
